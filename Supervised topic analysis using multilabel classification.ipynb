{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised topic analysis using multilabel classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary classifiers seek to predict a single optimal label for each feature vector. Typically, the values 0 or 1 are used to represent the label, although other schemes, such as True/False can be used.\n",
    "\n",
    "A multilabel classifier seeks to predict the optimal labels for each feature vector where the target is no longer a single value, as with a binary classifier, but a vector of binary labels.  For example, consider the problem of classifying images that can contain any of the letters A, B and C. The classifier is trained to recognise the letters and when shown an image with the letters A and C it should predict the labels to be $[1, 0, 1]$.\n",
    "\n",
    "This Jupyter notebook explores the problem of predicting the topics present in a corpus where training data exists as to which topics are present in a given document. A single document can contain words from zero or more topics. This is different from unsupervised topic analysis in that the topics have been defined and the topics present in a document are available, thus a supervised approach can be used. For simplicity, this notebook generates random text data and then different multilabel classification techniques are explored. This allows the relative proportions of the topics present in the texts to be changed and the effect on performance to be measured. The texts generated are not human readable because the generative model is simplistic, but it is sufficient for testing and demonstrating a multilabel classification solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document is generated using a mixture model. There are three primary components:\n",
    "\n",
    "* Stop words -- high frequency words such as 'a', 'the', 'see'.\n",
    "* Common words -- common English words that are not stop words, such as 'matter', 'found', 'create'.\n",
    "* Topic words -- words derived from a given topic.\n",
    "\n",
    "This work assumes that there are at least two topics of interest. In the code below, three random topics were defined: 'vehicle'; 'internet'; and 'garden'. The topics were chosen because they don't contain overlapping words.\n",
    "\n",
    "The number of stop words in a text $N_{stop}$ is determined by sampling from a Poisson distribution with mean $\\lambda_{stop}$. The number of stop words $N_{stop}$ required for the text are then sampled by randomly selecting stop words (with replacement), which are contained in a pre-defined set.\n",
    "\n",
    "Similarly, the number of common words in a text is determined by sampling from a Poisson distribution with mean $\\lambda_{common}$. $N_{common}$ words are randomly sampled with replacement from the set of all common words.\n",
    "\n",
    "The text can contain words from zero or more topics. The probability that topic $i$ appears in a text is denoted $p_i$ where $0 \\leq p_i \\leq 1$. The presence of the topic in a text is determined by sampling from a Bernoulli distribution with $p_i$. If a topic exists, the number of words randomly selected from the topic (with replacement) is determined by sampling from a Poisson distribution with mean $\\lambda_i$ where $\\lambda_i > 0$.\n",
    "\n",
    "In the model decribed, the topics of interest are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are commonly occurring words that are essentially 'noise'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423\n"
     ]
    }
   ],
   "source": [
    "# Stop words\n",
    "stop_words = ['a', 'about', 'above', 'across', 'after', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'among', 'an', 'and', 'another', 'any', 'anybody', 'anyone', 'anything', 'anywhere', 'are', 'area', 'areas', 'around', 'as', 'ask', 'asked', 'asking', 'asks', 'at', 'away', 'b', 'back', 'backed', 'backing', 'backs', 'be', 'became', 'because', 'become', 'becomes', 'been', 'before', 'began', 'behind', 'being', 'beings', 'best', 'better', 'between', 'big', 'both', 'but', 'by', 'c', 'came', 'can', 'cannot', 'case', 'cases', 'certain', 'certainly', 'clear', 'clearly', 'come', 'could', 'd', 'did', 'differ', 'different', 'differently', 'do', 'does', 'done', 'down', 'down', 'downed', 'downing', 'downs', 'during', 'e', 'each', 'early', 'either', 'end', 'ended', 'ending', 'ends', 'enough', 'even', 'evenly', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'f', 'face', 'faces', 'fact', 'facts', 'far', 'felt', 'few', 'find', 'finds', 'first', 'for', 'four', 'from', 'full', 'fully', 'further', 'furthered', 'furthering', 'furthers', 'g', 'gave', 'general', 'generally', 'get', 'gets', 'give', 'given', 'gives', 'go', 'going', 'good', 'goods', 'got', 'great', 'greater', 'greatest', 'group', 'grouped', 'grouping', 'groups', 'h', 'had', 'has', 'have', 'having', 'he', 'her', 'here', 'herself', 'high', 'high', 'high', 'higher', 'highest', 'him', 'himself', 'his', 'how', 'however', 'i', 'if', 'important', 'in', 'interest', 'interested', 'interesting', 'interests', 'into', 'is', 'it', 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kind', 'knew', 'know', 'known', 'knows', 'l', 'large', 'largely', 'last', 'later', 'latest', 'least', 'less', 'let', 'lets', 'like', 'likely', 'long', 'longer', 'longest', 'm', 'made', 'make', 'making', 'man', 'many', 'may', 'me', 'member', 'members', 'men', 'might', 'more', 'most', 'mostly', 'mr', 'mrs', 'much', 'must', 'my', 'myself', 'n', 'necessary', 'need', 'needed', 'needing', 'needs', 'never', 'new', 'new', 'newer', 'newest', 'next', 'no', 'nobody', 'non', 'noone', 'not', 'nothing', 'now', 'nowhere', 'number', 'numbers', 'o', 'of', 'off', 'often', 'old', 'older', 'oldest', 'on', 'once', 'one', 'only', 'open', 'opened', 'opening', 'opens', 'or', 'order', 'ordered', 'ordering', 'orders', 'other', 'others', 'our', 'out', 'over', 'p', 'part', 'parted', 'parting', 'parts', 'per', 'perhaps', 'place', 'places', 'point', 'pointed', 'pointing', 'points', 'possible', 'present', 'presented', 'presenting', 'presents', 'problem', 'problems', 'put', 'puts', 'q', 'quite', 'r', 'rather', 'really', 'right', 'right', 'room', 'rooms', 's', 'said', 'same', 'saw', 'say', 'says', 'second', 'seconds', 'see', 'seem', 'seemed', 'seeming', 'seems', 'sees', 'several', 'shall', 'she', 'should', 'show', 'showed', 'showing', 'shows', 'side', 'sides', 'since', 'small', 'smaller', 'smallest', 'so', 'some', 'somebody', 'someone', 'something', 'somewhere', 'state', 'states', 'still', 'still', 'such', 'sure', 't', 'take', 'taken', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'therefore', 'these', 'they', 'thing', 'things', 'think', 'thinks', 'this', 'those', 'though', 'thought', 'thoughts', 'three', 'through', 'thus', 'to', 'today', 'together', 'too', 'took', 'toward', 'turn', 'turned', 'turning', 'turns', 'two', 'u', 'under', 'until', 'up', 'upon', 'us', 'use', 'used', 'uses', 'v', 'very', 'w', 'want', 'wanted', 'wanting', 'wants', 'was', 'way', 'ways', 'we', 'well', 'wells', 'went', 'were', 'what', 'when', 'where', 'whether', 'which', 'while', 'who', 'whole', 'whose', 'why', 'will', 'with', 'within', 'without', 'work', 'worked', 'working', 'works', 'would', 'x', 'y', 'year', 'years', 'yet', 'you', 'young', 'younger', 'youngest', 'your', 'yours', 'z']\n",
    "stop_words = set(stop_words)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common words occur frequently, but not so frequently as to be deemed stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770\n"
     ]
    }
   ],
   "source": [
    "# Common words (non-stop words) derived from https://gist.github.com/deekayen/4148741\n",
    "common_words = {'charge', 'view', 'morning', 'especially', 'care', 'happy', 'wrote', 'occur', 'final', 'women', 'offer', 'rain', 'clean', 'nine', 'blow', 'excite', 'shape', 'teach', 'floor', 'black', 'copy', 'car', 'drink', 'quotient', 'practice', 'animal', 'grow', 'proper', 'believe', 'huge', 'segment', 'answer', 'begin', 'force', 'region', 'drive', 'contain', 'house', 'girl', 'board', 'coat', 'clothe', 'energy', 'visit', 'common', 'arrive', 'correct', 'difficult', 'color', 'song', 'person', 'dark', 'exercise', 'flower', 'caught', 'throw', 'anger', 'safe', 'spoke', \"won't\", 'head', 'paper', 'leave', 'neck', 'captain', 'except', 'trade', 'children', 'insect', 'mean', 'field', 'smile', 'voice', 'experience', 'sharp', 'protect', 'store', 'run', 'degree', 'flow', 'control', 'record', 'main', 'syllable', 'left', 'moment', 'ready', 'born', 'table', 'rest', 'winter', 'million', 'string', 'picture', 'example', 'sail', 'party', 'stand', 'true', 'boy', 'beauty', 'front', 'solution', 'cold', 'jump', 'soon', 'triangle', 'study', 'plan', 'special', 'fine', 'appear', 'position', 'type', 'truck', 'tree', 'fall', 'mountain', 'art', 'chart', 'planet', 'set', 'process', 'period', 'broad', 'miss', 'change', 'bright', 'enter', 'clock', 'follow', 'design', 'hill', 'touch', 'consider', 'sugar', 'team', 'third', 'pitch', 'provide', 'story', 'town', 'sight', 'afraid', 'cry', 'subject', 'level', 'please', 'river', 'match', 'held', 'life', 'written', 'help', 'station', 'vary', 'chick', 'die', 'sign', 'lone', 'develop', 'valley', 'rub', 'add', 'law', 'brother', 'hard', 'natural', 'tone', 'land', 'bring', 'break', 'opposite', 'piece', 'wild', 'nor', 'distant', 'indicate', 'dear', 'surprise', 'wall', 'reply', 'neighbor', 'lift', 'circle', 'food', 'moon', 'earth', 'learn', 'loud', 'school', 'shout', 'notice', 'hat', 'sell', 'subtract', 'company', 'pay', 'chance', 'mark', 'material', 'wash', 'range', 'probable', 'lot', 'row', 'gather', 'week', 'lie', 'suffix', 'dance', 'oxygen', 'prepare', 'poor', 'window', 'white', 'nose', 'speech', 'own', 'bread', 'melody', 'idea', 'flat', 'coast', 'gun', 'cat', 'multiply', 'poem', 'double', 'wind', 'metal', 'bit', 'friend', 'stood', 'egg', 'slave', 'sand', 'child', 'real', 'door', 'fresh', 'question', 'shoulder', 'pick', 'thick', 'stay', 'listen', 'seven', 'salt', 'forward', 'forest', 'dead', 'fly', 'locate', 'water', 'shore', 'cost', 'mount', 'numeral', 'feed', 'market', 'ten', 'hunt', 'hour', 'glass', 'cross', 'mass', 'blood', 'scale', 'continent', 'shop', 'look', 'class', 'roll', 'round', 'decimal', 'chord', 'reason', 'chief', 'simple', 'office', 'sent', 'join', 'mix', 'hair', 'finish', 'stick', 'red', 'light', 'dog', 'grand', 'substance', 'wrong', 'sheet', 'unit', 'property', 'live', 'speed', 'symbol', 'drop', 'vowel', 'heat', 'remember', 'shine', 'noon', 'race', 'dad', 'line', 'system', 'noun', 'woman', 'horse', 'blue', 'garden', 'figure', 'electric', 'weight', 'lake', 'warm', 'division', 'call', 'village', 'nation', 'receive', 'character', 'tall', 'soldier', 'reach', 'matter', 'steel', 'found', 'street', 'bat', 'buy', 'motion', 'five', 'soil', 'imagine', 'play', 'score', 'trip', 'skin', 'spot', 'dream', 'pass', 'bird', 'pound', 'crease', 'cow', 'mother', 'wonder', 'watch', 'length', 'section', 'science', 'happen', 'raise', 'eat', 'cloud', 'grew', 'move', 'read', 'stead', 'guess', 'fire', 'milk', 'able', 'tiny', 'road', 'swim', 'shoe', 'wire', 'heard', 'hope', 'block', 'compare', 'human', 'direct', 'sky', 'seed', 'steam', 'allow', 'support', 'form', 'yes', 'bank', 'single', 'claim', 'history', 'list', 'talk', 'strong', 'root', 'produce', 'sat', 'stream', 'cool', 'late', 'space', 'current', 'spell', 'world', 'father', 'ship', 'plural', 'object', 'magnet', 'sun', 'rail', 'else', 'box', 'molecule', 'act', 'wheel', 'job', 'fast', 'cause', 'leg', 'sister', 'gentle', 'desert', 'fell', 'condition', 'south', 'middle', 'guide', 'train', 'son', 'bed', 'chair', 'cotton', 'oh', 'cell', 'enemy', 'self', 'log', 'tire', 'fill', 'power', 'test', 'grass', 'fun', 'ball', 'bear', 'north', 'hundred', 'center', 'modern', 'start', 'inch', 'season', 'top', 'rock', 'deep', 'quick', 'arm', 'stretch', 'path', 'broke', 'teeth', 'near', 'suggest', 'evening', 'check', 'cut', 'plain', 'catch', 'key', 'gray', 'doctor', 'result', 'fit', 'success', 'ring', 'gold', 'noise', 'instrument', 'plant', 'meet', 'basic', 'paint', 'element', 'arrange', 'apple', 'govern', 'green', 'cover', 'expect', 'rise', 'game', 'joy', 'lay', 'burn', 'atom', 'rule', 'thin', 'discuss', 'note', 'fair', 'feel', 'spread', 'trouble', 'smell', 'favor', 'snow', 'mind', 'fish', 'wish', 'divide', 'lead', 'meant', 'name', 'pattern', 'decide', 'ran', 'finger', 'strange', 'determine', 'connect', 'share', 'parent', 'dollar', 'repeat', 'time', 'family', 'settle', 'ocean', 'sudden', 'camp', 'nature', 'particular', 'paragraph', 'operate', 'tube', 'short', 'ground', 'bar', 'colony', 'kill', 'wing', 'night', 'pose', 'master', 'told', 'hand', 'create', 'track', 'pretty', 'wear', 'west', 'iron', 'air', 'corner', 'silent', 'close', 'century', 'observe', 'glad', 'collect', 'suit', 'gas', 'sentence', 'phrase', 'beat', 'famous', 'hold', 'music', 'star', 'spring', 'size', 'surface', 'walk', 'stone', 'square', 'organ', 'count', 'cent', 'little', 'hot', 'danger', 'yard', 'equate', 'consonant', 'love', 'weather', 'hit', 'depend', 'major', 'represent', 'solve', 'continue', 'require', \"don't\", 'pull', 'include', 'describe', 'engine', 'instant', 'wave', 'oil', 'carry', 'verb', 'machine', 'summer', 'ear', 'deal', 'wife', 'fruit', 'seat', 'stop', 'language', 'soft', 'write', 'slip', 'people', 'fear', 'sing', 'heart', 'build', 'corn', 'free', 'bottom', 'value', 'six', 'band', 'rose', 'foot', 'slow', 'letter', 'money', 'dress', 'book', 'country', 'day', 'wood', 'low', 'sleep', 'mile', 'dry', 'wide', 'am', 'capital', 'bell', 'invent', 'fat', 'product', 'hear', 'temperature', 'thank', 'similar', 'tool', 'led', 'bought', 'bone', 'tail', 'agree', 'climb', 'prove', 'straight', 'death', 'lost', 'shell', 'industry', 'war', 'rich', 'win', 'mine', 'laugh', 'sound', 'original', 'brought', 'month', 'crop', 'draw', 'half', 'island', 'body', 'student', 'eight', 'age', 'supply', 'past', 'experiment', 'complete', 'ago', 'page', 'mouth', 'wait', 'plane', 'edge', 'rope', 'ice', 'usual', 'quiet', 'speak', 'spend', 'yellow', 'silver', 'radio', 'farm', 'dictionary', 'quart', 'meat', 'total', 'populate', 'fight', 'base', 'print', 'sea', 'exact', 'method', 'ride', 'eye', 'home', 'baby', 'sense', 'equal', 'boat', 'cook', 'map', 'word', 'minute', 'separate', 'serve', 'gone', 'twenty', 'term', 'fig', 'post', 'thousand', 'hole', 'press', 'liquid', 'choose', 'east', 'sit', 'column', 'travel', 'port', 'duck', 'course', 'try', 'brown', 'city', 'king', 'ease', 'skill', 'card', 'fraction', 'bad', 'lady', 'measure', 'busy', 'kept', 'send', 'heavy', 'search', 'tie', 'crowd', 'push', 'feet', 'pair', 'tell', 'select', 'branch', 'event', 'hurry', 'effect', 'step', 'save'}\n",
    "print(len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words and common words should be mutually exclusive\n",
    "assert len(stop_words.intersection(common_words)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words for three topics are now defined. The words or phrases for the topics were created from https://relatedwords.org/relatedto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "# Vehicle words (class 0)\n",
    "vehicle_words = {'motorcycle', 'headlight', 'climate change', 'glove compartment', 'coupe', 'motor vehicle', 'electric car', 'hatchback', 'sport utility vehicle', 'cab', 'jeep', 'van', 'car horn', 'truck', 'stock car', 'automobile', 'bumper', 'passenger', 'diesel', 'rear window', 'high gear', 'taxi', 'vehicle', 'motorcar', 'bus', 'suv', 'roadster', 'limousine', 'mercedes', 'alternator', 'backseat', 'air pollution', 'motor', 'wheel', 'garage'}\n",
    "print(len(vehicle_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "# Internet words (class 1)\n",
    "internet_words = {'intranet', 'ftp', 'instant messaging', 'offline', 'web', 'google', 'web server', 'blogging', 'hyperlink', 'voip', 'ip address', 'websites', 'online', 'facebook', 'file sharing', 'twitter', 'www', 'youtube', 'broadband', 'world wide web', 'cyber', 'microsoft', 'social network', 'media', 'email', 'myspace', 'webcam', 'peer-to-peer', 'computer network', 'website', 'web page', 'web site', 'web browser', 'multimedia', 'cyberspace', 'http'}\n",
    "print(len(internet_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "# Garden words (class 2)\n",
    "garden_words = {'yard', 'horticultural', 'plant', 'rose garden', 'flower', 'landscape', 'patio', 'nature', 'compost heap', 'gardening', 'courtyard', 'rockery', 'tree', 'terrace', 'verbena', 'grass', 'hellebore', 'meadow', 'flower garden', 'park', 'botanical garden', 'pergola', 'hydrangea', 'land', 'herb garden', 'landscaping', 'gardener', 'ornamental', 'rosebush', 'backyard', 'wildflower', 'fountain', 'topiary', 'vegetable garden', 'orchard', 'lawn', 'kitchen garden', 'agriculture'}\n",
    "print(len(garden_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "761\n"
     ]
    }
   ],
   "source": [
    "# Remove topic words from the common words\n",
    "common_words = common_words.difference(vehicle_words).difference(internet_words).difference(garden_words)\n",
    "print(len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens for each class\n",
    "class_words = [\n",
    "    vehicle_words,\n",
    "    internet_words,\n",
    "    garden_words\n",
    "]\n",
    "\n",
    "# Probability of tokens from the class appearing\n",
    "p_class = [0.4, 0.2, 0.05]\n",
    "assert len(class_words) == len(p_class)\n",
    "\n",
    "# Mean number of words from each class\n",
    "lambda_class = [2, 1, 3]\n",
    "assert len(lambda_class) == len(p_class)\n",
    "\n",
    "# Mean number of stopwords appearing\n",
    "lambda_stop = 2\n",
    "\n",
    "# Mean number of common words appearing\n",
    "lambda_common = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_elements_from_set(s, n):\n",
    "    \"\"\"Returns n random elements from set s (with replacement).\"\"\"\n",
    "    \n",
    "    # Preconditions\n",
    "    assert isinstance(s, set), f\"Expected a set, got {type(s)}\"\n",
    "    assert isinstance(n, int), f\"Expected an int, got {type(n)}\"\n",
    "    assert 0 <= n, f\"Invalid number of elements {n}\"\n",
    "    \n",
    "    elements = [random.choice(list(s)) for _ in range(n)]\n",
    "    \n",
    "    # Postcondition\n",
    "    assert len(elements) == n\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_text():\n",
    "    \"\"\"Generates a random piece of text.\"\"\"\n",
    "    \n",
    "    # Stop words\n",
    "    text = random_elements_from_set(stop_words, np.random.poisson(lambda_stop))\n",
    "    \n",
    "    # Common words\n",
    "    text.extend(random_elements_from_set(common_words, np.random.poisson(lambda_common)))\n",
    "    \n",
    "    # Words from each class\n",
    "    class_present = []\n",
    "    for i in range(len(class_words)):\n",
    "        \n",
    "        if random.random() <= p_class[i]:\n",
    "            \n",
    "            topic_words = []\n",
    "            while len(topic_words) == 0:\n",
    "                topic_words = random_elements_from_set(class_words[i], np.random.poisson(lambda_class[i]))\n",
    "                \n",
    "            text.extend(topic_words)\n",
    "            class_present.append(1)\n",
    "        else:\n",
    "            class_present.append(0)\n",
    "        \n",
    "    # Postconditions\n",
    "    assert isinstance(text, list)\n",
    "    assert len(class_present) == len(class_words)\n",
    "    \n",
    "    return text, class_present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['does', 'interesting', 'summer', 'electric car', 'climate change'],\n",
       " [1, 0, 0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show an example of what the generate_random_text() function produces\n",
    "generate_random_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the corpus is generated, which is a collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of synthetic documents to generate\n",
    "n_documents = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus(n_docs):\n",
    "    \"\"\"Generate a corpus of documents.\"\"\"\n",
    "    \n",
    "    # Preconditions\n",
    "    assert isinstance(n_docs, int)\n",
    "    assert n_docs > 0\n",
    "    \n",
    "    texts = []\n",
    "    class_labels = []\n",
    "    \n",
    "    for i in range(n_docs):\n",
    "        words, labels = generate_random_text()\n",
    "        \n",
    "        # The topic can contain multiple words so this step separates them\n",
    "        tokens = \" \".join(words)\n",
    "        texts.append(tokens)\n",
    "        class_labels.append(labels)\n",
    "        \n",
    "    class_labels = np.array(class_labels)\n",
    "    \n",
    "    # Postconditions\n",
    "    assert isinstance(texts, list)\n",
    "    assert len(texts) == n_docs\n",
    "    assert class_labels.shape == (n_docs, len(class_words))\n",
    "    \n",
    "    return texts, class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, class_labels = generate_corpus(n_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinks enough check dictionary especially online hyperlink\n",
      "we above across along possible listen glove compartment glove compartment multimedia cyber\n"
     ]
    }
   ],
   "source": [
    "# Show examples of texts\n",
    "print(texts[0])\n",
    "print(texts[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the number of tokens in a document is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the simplistic way the texts are constructed a simple white space tokeniser can be used\n",
    "number_of_tokens = [len(t.split(\" \")) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(number_of_tokens, bins=np.arange(max(number_of_tokens)+1) - 0.5)\n",
    "plt.xlabel('Number of tokens in a document')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature matrix generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to convert the textual data to numeric values by counting the number of times a token appears in each document. Stop words are ignored because they are deemed 'noise' and don't provide information as to which topic a text contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise, remove stop words and count token occurrences\n",
    "vectorizer = CountVectorizer(stop_words=list(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts shape = (1000, 818)\n",
      "y shape = (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "counts = vectorizer.fit_transform(texts)\n",
    "print(f\"counts shape = {counts.shape}\")\n",
    "\n",
    "y = class_labels\n",
    "print(f\"y shape = {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of rows\n",
    "assert counts.shape[0] == n_documents\n",
    "\n",
    "# Check none of the stop words have been counted\n",
    "assert len(set(vectorizer.get_feature_names()).intersection(stop_words)) == 0\n",
    "\n",
    "# Check the shape of the labels\n",
    "assert y.shape[0] == n_documents\n",
    "assert y.shape[1] == len(class_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counts are normalised by converting the values to TF-IDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape = (1000, 818)\n"
     ]
    }
   ],
   "source": [
    "# Convert the counts to TF-IDF scores\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "X = transformer.fit_transform(counts)\n",
    "print(f\"X shape = {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into test and training sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature matrix and class matrix are together now divided into training and test sets. The training set will be used for training the classifiers. The test set will only be used at the end to measure the performance of the chosen classifer. The performance on the test set gives an indication as to how well the classifier will work on unseen data provided it is sufficiently similar to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial classifier selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section a range of multilabel classifiers are checked to see whether they show promising performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-Nearest neighbour classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier()\n",
    "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005148005148005148"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, y_train_knn_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier()\n",
    "y_train_dt_pred = cross_val_predict(dt_clf, X_train, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7382939468829038"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, y_train_dt_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rf_pred = cross_val_predict(rf_clf, X_train, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2868271402975773"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, y_train_rf_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most promising classifier from the previous section is now tuned by performing a parameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_depth': [6, 8, 10, 12, 14, 18, 20, None]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(dt_clf, param_grid, cv=3,\n",
    "                           scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=DecisionTreeClassifier(),\n",
       "             param_grid=[{'criterion': ['gini', 'entropy'],\n",
       "                          'max_depth': [6, 8, 10, 12, 14, 18, 20, None],\n",
       "                          'splitter': ['best', 'random']}],\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'max_depth': None, 'splitter': 'random'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9073672566371681\n",
      "Recall: 0.649256086002195\n",
      "f1-score: 0.746911663216011\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation using the best estimator\n",
    "y_train_pred = cross_val_predict(grid_search.best_estimator_, X_train, y_train, cv=3)\n",
    "\n",
    "print(f\"Precision: {precision_score(y_train, y_train_pred, average='macro')}\")\n",
    "print(f\"Recall: {recall_score(y_train, y_train_pred, average='macro')}\")\n",
    "print(f\"f1-score: {f1_score(y_train, y_train_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the optimal classifier has been determined from the possible classifiers given the range of parameters, the features that should be used are now selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.01418693, 0.        ,\n",
       "       0.        , 0.00211167, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The estimator produces scores for the features\n",
    "grid_search.best_estimator_.feature_importances_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features based on the model\n",
    "model = SelectFromModel(grid_search.best_estimator_, prefit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<670x818 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2530 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<670x84 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 903 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note how the number of features is reduced by this selection phase\n",
    "X_train_fs = model.transform(X_train)\n",
    "X_train_fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal classifier given the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=DecisionTreeClassifier(),\n",
       "             param_grid=[{'criterion': ['gini', 'entropy'],\n",
       "                          'max_depth': [6, 8, 10, 12, 14, 18, 20, None],\n",
       "                          'splitter': ['best', 'random']}],\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A grid search is now performed again, this time with the training data with the\n",
    "# reduced set of features\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_depth': [6, 8, 10, 12, 14, 18, 20, None]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(dt_clf, param_grid, cv=3,\n",
    "                           scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train_fs, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': None, 'splitter': 'random'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9069306930693068\n",
      "Recall: 0.7321385812630948\n",
      "f1-score: 0.8097936856626031\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation using the best estimator\n",
    "y_train_pred = cross_val_predict(grid_search.best_estimator_, X_train_fs, y_train, cv=3)\n",
    "\n",
    "print(f\"Precision: {precision_score(y_train, y_train_pred, average='macro')}\")\n",
    "print(f\"Recall: {recall_score(y_train, y_train_pred, average='macro')}\")\n",
    "print(f\"f1-score: {f1_score(y_train, y_train_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final performance measure on held-out test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the performance of the chosen classifier on the held-out test set is determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<330x84 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 426 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retain the selected features\n",
    "X_test_fs = model.transform(X_test)\n",
    "X_test_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7423899319279754"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier using all of the available training data\n",
    "grid_search.best_estimator_.fit(X_train_fs, y_train)\n",
    "\n",
    "# Predict the labels based on the test data set's feature matrix\n",
    "y_pred = grid_search.best_estimator_.predict(X_test_fs)\n",
    "\n",
    "# Measure the performance of the classifier on the test set\n",
    "f1_score(y_test, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9134615384615384"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6392216548834774"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
